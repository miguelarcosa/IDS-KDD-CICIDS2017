{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95d3ae0-f82d-4652-a3cd-0dd8f3e06428",
   "metadata": {},
   "source": [
    "# NSK-KDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c9021c-587b-4222-a4f1-071a52ae61b8",
   "metadata": {},
   "source": [
    "## Logistic Regression (no SMOTE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be38592b-b2fb-42c5-b6fd-f2c13bf27dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classification Metrics ===\n",
      "Accuracy      : 0.7443\n",
      "Precision     : 0.9142\n",
      "Recall        : 0.6079\n",
      "F1-Score      : 0.7302\n",
      "FAR      : 0.0754\n",
      "AUC      : 0.8276\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "TN: 8978, FP: 732\n",
      "FN: 5032, TP: 7800\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# === 0. Fix random seed for reproducibility ===\n",
    "np.random.seed(42)\n",
    "\n",
    "# === 1. Load binary-labeled train and test datasets ===\n",
    "train_df = pd.read_csv(\"KDDTrain_cleaned_binary.csv\")\n",
    "test_df = pd.read_csv(\"KDDTest_cleaned_binary.csv\")\n",
    "\n",
    "# === 2. Split features and labels ===\n",
    "X_train = train_df.iloc[:, :-1]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "\n",
    "# === 3. One-hot encoding of categorical variables ===\n",
    "X_all = pd.concat([X_train, X_test])\n",
    "X_all_encoded = pd.get_dummies(X_all, drop_first=True)\n",
    "\n",
    "X_train_encoded = X_all_encoded.iloc[:len(X_train), :]\n",
    "X_test_encoded = X_all_encoded.iloc[len(X_train):, :]\n",
    "\n",
    "# === 4. Normalize features ===\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "# === 5. Train logistic regression model ===\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === 6. Predict and calculate metrics ===\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]  # For AUC\n",
    "\n",
    "# === 7. Confusion matrix and metrics ===\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "far = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Alarm Rate\n",
    "\n",
    "# === 8. Print results ===\n",
    "print(\"=== Classification Metrics ===\")\n",
    "print(f\"Accuracy      : {accuracy:.4f}\")\n",
    "print(f\"Precision     : {precision:.4f}\")\n",
    "print(f\"Recall        : {recall:.4f}\")\n",
    "print(f\"F1-Score      : {f1:.4f}\")\n",
    "print(f\"FAR      : {far:.4f}\")\n",
    "print(f\"AUC      : {auc:.4f}\")\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(f\"TN: {tn}, FP: {fp}\")\n",
    "print(f\"FN: {fn}, TP: {tp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d220bb4-0adb-4676-b800-a3c816e59f05",
   "metadata": {},
   "source": [
    "## Logistic Regression (with SMOTE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8de12993-988b-4998-b887-37cbb7294855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classification Metrics (with SMOTE) ===\n",
      "Accuracy      : 0.7458\n",
      "Precision     : 0.9137\n",
      "Recall        : 0.6112\n",
      "F1-Score      : 0.7324\n",
      "AUC           : 0.8227\n",
      "FAR (False Alarm Rate): 0.0763\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "TN: 8969, FP: 741\n",
      "FN: 4989, TP: 7843\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# === 0. Fix random seed for reproducibility ===\n",
    "np.random.seed(42)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# === 1. Load binary-labeled datasets ===\n",
    "train_df = pd.read_csv(\"KDDTrain_cleaned_binary.csv\")\n",
    "test_df = pd.read_csv(\"KDDTest_cleaned_binary.csv\")\n",
    "\n",
    "# === 2. Split features and labels ===\n",
    "X_train = train_df.iloc[:, :-1]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "\n",
    "# === 3. One-hot encode categorical columns ===\n",
    "X_all = pd.concat([X_train, X_test])\n",
    "X_all_encoded = pd.get_dummies(X_all, drop_first=True)\n",
    "\n",
    "X_train_encoded = X_all_encoded.iloc[:len(X_train), :]\n",
    "X_test_encoded = X_all_encoded.iloc[len(X_train):, :]\n",
    "\n",
    "# === 4. Normalize features ===\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "# === 5. Apply SMOTE to training data only ===\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# === 6. Train logistic regression model ===\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# === 7. Predict and calculate metrics ===\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "far = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "# === 8. Print results ===\n",
    "print(\"=== Classification Metrics (with SMOTE) ===\")\n",
    "print(f\"Accuracy      : {accuracy:.4f}\")\n",
    "print(f\"Precision     : {precision:.4f}\")\n",
    "print(f\"Recall        : {recall:.4f}\")\n",
    "print(f\"F1-Score      : {f1:.4f}\")\n",
    "print(f\"AUC           : {auc:.4f}\")\n",
    "print(f\"FAR (False Alarm Rate): {far:.4f}\")\n",
    "\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(f\"TN: {tn}, FP: {fp}\")\n",
    "print(f\"FN: {fn}, TP: {tp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a94c9-f9a2-49f3-a15a-199ed9a01bbb",
   "metadata": {},
   "source": [
    "## Naive Bayes (no SMOTE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7b1dbf7-5d1f-4727-9b6b-3ae363d8b05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classification Metrics (Naive Bayes, No SMOTE) ===\n",
      "Accuracy      : 0.5649\n",
      "Precision     : 0.9800\n",
      "Recall        : 0.2406\n",
      "F1-Score      : 0.3864\n",
      "AUC           : 0.7987\n",
      "FAR (False Alarm Rate): 0.0065\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "TN: 9647, FP: 63\n",
      "FN: 9744, TP: 3088\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# === 0. Fix random seed for reproducibility ===\n",
    "np.random.seed(42)\n",
    "\n",
    "# === 1. Load binary-labeled datasets ===\n",
    "train_df = pd.read_csv(\"KDDTrain_cleaned_binary.csv\")\n",
    "test_df = pd.read_csv(\"KDDTest_cleaned_binary.csv\")\n",
    "\n",
    "# === 2. Split features and labels ===\n",
    "X_train = train_df.iloc[:, :-1]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "\n",
    "# === 3. One-hot encode categorical columns ===\n",
    "X_all = pd.concat([X_train, X_test])\n",
    "X_all_encoded = pd.get_dummies(X_all, drop_first=True)\n",
    "\n",
    "X_train_encoded = X_all_encoded.iloc[:len(X_train), :]\n",
    "X_test_encoded = X_all_encoded.iloc[len(X_train):, :]\n",
    "\n",
    "# === 4. Normalize features ===\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "# === 5. Train Naive Bayes model ===\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === 6. Predict and evaluate ===\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "far = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "# === 7. Print results ===\n",
    "print(\"=== Classification Metrics (Naive Bayes, No SMOTE) ===\")\n",
    "print(f\"Accuracy      : {accuracy:.4f}\")\n",
    "print(f\"Precision     : {precision:.4f}\")\n",
    "print(f\"Recall        : {recall:.4f}\")\n",
    "print(f\"F1-Score      : {f1:.4f}\")\n",
    "print(f\"AUC           : {auc:.4f}\")\n",
    "print(f\"FAR (False Alarm Rate): {far:.4f}\")\n",
    "\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(f\"TN: {tn}, FP: {fp}\")\n",
    "print(f\"FN: {fn}, TP: {tp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445fe0e7-ddb7-4bd9-9b3d-05ef216898cc",
   "metadata": {},
   "source": [
    "## Naive Bayes (with SMOTE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "048d0071-964e-476c-9ec0-863963d4361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classification Metrics (Naive Bayes with SMOTE) ===\n",
      "Accuracy      : 0.5650\n",
      "Precision     : 0.9800\n",
      "Recall        : 0.2408\n",
      "F1-Score      : 0.3866\n",
      "AUC           : 0.7978\n",
      "FAR (False Alarm Rate): 0.0065\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "TN: 9647, FP: 63\n",
      "FN: 9742, TP: 3090\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "\n",
    "# === 0. Fix random seed for reproducibility ===\n",
    "np.random.seed(42)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# === 1. Load binary-labeled datasets ===\n",
    "train_df = pd.read_csv(\"KDDTrain_cleaned_binary.csv\")\n",
    "test_df = pd.read_csv(\"KDDTest_cleaned_binary.csv\")\n",
    "\n",
    "# === 2. Split features and labels ===\n",
    "X_train = train_df.iloc[:, :-1]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "\n",
    "# === 3. One-hot encode categorical columns ===\n",
    "X_all = pd.concat([X_train, X_test])\n",
    "X_all_encoded = pd.get_dummies(X_all, drop_first=True)\n",
    "\n",
    "X_train_encoded = X_all_encoded.iloc[:len(X_train), :]\n",
    "X_test_encoded = X_all_encoded.iloc[len(X_train):, :]\n",
    "\n",
    "# === 4. Normalize features ===\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "# === 5. Apply SMOTE to training data only ===\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# === 6. Train Naive Bayes model ===\n",
    "model = GaussianNB()\n",
    "model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# === 7. Predict and evaluate ===\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_proba = model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "far = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "# === 8. Print results ===\n",
    "print(\"=== Classification Metrics (Naive Bayes with SMOTE) ===\")\n",
    "print(f\"Accuracy      : {accuracy:.4f}\")\n",
    "print(f\"Precision     : {precision:.4f}\")\n",
    "print(f\"Recall        : {recall:.4f}\")\n",
    "print(f\"F1-Score      : {f1:.4f}\")\n",
    "print(f\"AUC           : {auc:.4f}\")\n",
    "print(f\"FAR (False Alarm Rate): {far:.4f}\")\n",
    "\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(f\"TN: {tn}, FP: {fp}\")\n",
    "print(f\"FN: {fn}, TP: {tp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3016c8-b21f-42ff-b85a-18b2497e1676",
   "metadata": {},
   "source": [
    "## Autoencoder (no SMOTE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed8eb349-b924-4978-8474-37376c067f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression on AE Embedding ===\n",
      "Accuracy      : 0.7617\n",
      "Precision     : 0.9165\n",
      "Recall        : 0.6397\n",
      "F1-Score      : 0.7534\n",
      "AUC           : 0.9040\n",
      "FAR           : 0.0770\n",
      "Confusion Matrix: TN=8962, FP=748, FN=4624, TP=8208\n",
      "\n",
      "=== Random Forest on AE Embedding ===\n",
      "Accuracy      : 0.7661\n",
      "Precision     : 0.9229\n",
      "Recall        : 0.6428\n",
      "F1-Score      : 0.7578\n",
      "AUC           : 0.9350\n",
      "FAR           : 0.0710\n",
      "Confusion Matrix: TN=9021, FP=689, FN=4583, TP=8249\n"
     ]
    }
   ],
   "source": [
    "# === Reproducibility setup (set BEFORE importing tensorflow) ===\n",
    "import os\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"        # stable hashing in Python\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"   # deterministic TF ops (CPU)\n",
    "# Optional: force CPU only for strict determinism across machines\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "# If available in your TF version, also enforce global determinism:\n",
    "try:\n",
    "    tf.config.experimental.enable_op_determinism(True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# === Libraries ===\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# === 1. Load datasets ===\n",
    "train_df = pd.read_csv(\"KDDTrain_cleaned_binary.csv\")\n",
    "test_df = pd.read_csv(\"KDDTest_cleaned_binary.csv\")\n",
    "\n",
    "X_train = train_df.iloc[:, :-1]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "\n",
    "# === 2. One-hot encoding ===\n",
    "X_all = pd.concat([X_train, X_test])\n",
    "X_all_encoded = pd.get_dummies(X_all, drop_first=True)\n",
    "\n",
    "X_train_encoded = X_all_encoded.iloc[:len(X_train), :]\n",
    "X_test_encoded = X_all_encoded.iloc[len(X_train):, :]\n",
    "\n",
    "# === 3. Normalize ===\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "# === 4. Build and train autoencoder (deterministic) ===\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "output_layer = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# Nota: shuffle=False + validation_split produce un split determinista (toma el último 10%).\n",
    "autoencoder.fit(\n",
    "    X_train_scaled, X_train_scaled,\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    shuffle=False,          # <- clave para reproducibilidad\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# === 5. Generate latent embeddings ===\n",
    "X_train_embedded = encoder.predict(X_train_scaled, verbose=0)\n",
    "X_test_embedded = encoder.predict(X_test_scaled, verbose=0)\n",
    "\n",
    "# === 6. Define evaluation function ===\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    metrics = {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"F1-Score\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba),\n",
    "        \"FAR\": fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"ConfusionMatrix\": (tn, fp, fn, tp)\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# === 7. Train and evaluate classifiers on embeddings ===\n",
    "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100, n_jobs=1)  # n_jobs=1 para reproducibilidad\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_model(\"Logistic Regression on AE Embedding\", logreg,\n",
    "                              X_train_embedded, y_train, X_test_embedded, y_test))\n",
    "results.append(evaluate_model(\"Random Forest on AE Embedding\", rf,\n",
    "                              X_train_embedded, y_train, X_test_embedded, y_test))\n",
    "\n",
    "# === 8. Print comparison ===\n",
    "for res in results:\n",
    "    print(f\"\\n=== {res['Model']} ===\")\n",
    "    print(f\"Accuracy      : {res['Accuracy']:.4f}\")\n",
    "    print(f\"Precision     : {res['Precision']:.4f}\")\n",
    "    print(f\"Recall        : {res['Recall']:.4f}\")\n",
    "    print(f\"F1-Score      : {res['F1-Score']:.4f}\")\n",
    "    print(f\"AUC           : {res['AUC']:.4f}\")\n",
    "    print(f\"FAR           : {res['FAR']:.4f}\")\n",
    "    tn, fp, fn, tp = res['ConfusionMatrix']\n",
    "    print(f\"Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a93226-637c-4d8e-952d-85e445380a8e",
   "metadata": {},
   "source": [
    "## Autoencoder (with SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2f75024e-d823-48cd-aeec-57b233870489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression + AE + SMOTE ===\n",
      "Accuracy      : 0.7654\n",
      "Precision     : 0.9166\n",
      "Recall        : 0.6467\n",
      "F1-Score      : 0.7583\n",
      "AUC           : 0.9043\n",
      "FAR           : 0.0778\n",
      "Confusion Matrix: TN=8955, FP=755, FN=4534, TP=8298\n",
      "\n",
      "=== Random Forest + AE + SMOTE ===\n",
      "Accuracy      : 0.7822\n",
      "Precision     : 0.9652\n",
      "Recall        : 0.6405\n",
      "F1-Score      : 0.7700\n",
      "AUC           : 0.9355\n",
      "FAR           : 0.0305\n",
      "Confusion Matrix: TN=9414, FP=296, FN=4613, TP=8219\n"
     ]
    }
   ],
   "source": [
    "# === 1. Reproducibility setup ===\n",
    "import os, random\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # Opcional: fuerza CPU\n",
    "\n",
    "random.seed(42)\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "try:\n",
    "    tf.config.experimental.enable_op_determinism(True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# === 2. Libraries ===\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# === 3. Load datasets ===\n",
    "train_df = pd.read_csv(\"KDDTrain_cleaned_binary.csv\")\n",
    "test_df = pd.read_csv(\"KDDTest_cleaned_binary.csv\")\n",
    "\n",
    "X_train = train_df.iloc[:, :-1]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "\n",
    "# === 4. One-hot encoding ===\n",
    "X_all = pd.concat([X_train, X_test])\n",
    "X_all_encoded = pd.get_dummies(X_all, drop_first=True)\n",
    "\n",
    "X_train_encoded = X_all_encoded.iloc[:len(X_train), :]\n",
    "X_test_encoded = X_all_encoded.iloc[len(X_train):, :]\n",
    "\n",
    "# === 5. Normalize ===\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "# === 6. Autoencoder definition ===\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "output_layer = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "\n",
    "# 🔑 shuffle=False para reproducibilidad (validation_split será determinista: último 10%)\n",
    "autoencoder.fit(\n",
    "    X_train_scaled, X_train_scaled,\n",
    "    epochs=20,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    validation_split=0.1,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# === 7. Embedding extraction ===\n",
    "X_train_embedded = encoder.predict(X_train_scaled, verbose=0)\n",
    "X_test_embedded = encoder.predict(X_test_scaled, verbose=0)\n",
    "\n",
    "# === 8. Apply SMOTE to embeddings ===\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_embedded_smote, y_train_smote = sm.fit_resample(X_train_embedded, y_train)\n",
    "\n",
    "# === 9. Evaluation function ===\n",
    "def evaluate_model(name, model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_test, y_pred, zero_division=0),\n",
    "        \"F1-Score\": f1_score(y_test, y_pred, zero_division=0),\n",
    "        \"AUC\": roc_auc_score(y_test, y_proba),\n",
    "        \"FAR\": fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"ConfusionMatrix\": (tn, fp, fn, tp)\n",
    "    }\n",
    "\n",
    "# === 10. Classifiers ===\n",
    "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100, n_jobs=1)  # n_jobs=1 -> reproducible\n",
    "\n",
    "results = []\n",
    "results.append(evaluate_model(\"Logistic Regression + AE + SMOTE\", logreg,\n",
    "                              X_train_embedded_smote, y_train_smote,\n",
    "                              X_test_embedded, y_test))\n",
    "\n",
    "results.append(evaluate_model(\"Random Forest + AE + SMOTE\", rf,\n",
    "                              X_train_embedded_smote, y_train_smote,\n",
    "                              X_test_embedded, y_test))\n",
    "\n",
    "# === 11. Print results ===\n",
    "for res in results:\n",
    "    print(f\"\\n=== {res['Model']} ===\")\n",
    "    print(f\"Accuracy      : {res['Accuracy']:.4f}\")\n",
    "    print(f\"Precision     : {res['Precision']:.4f}\")\n",
    "    print(f\"Recall        : {res['Recall']:.4f}\")\n",
    "    print(f\"F1-Score      : {res['F1-Score']:.4f}\")\n",
    "    print(f\"AUC           : {res['AUC']:.4f}\")\n",
    "    print(f\"FAR           : {res['FAR']:.4f}\")\n",
    "    tn, fp, fn, tp = res[\"ConfusionMatrix\"]\n",
    "    print(f\"Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0042be-6f58-48c0-b23f-65306c6cfb3d",
   "metadata": {},
   "source": [
    "### LDA (no SMOTE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d76426c8-5e4c-4f3e-9570-cddd67715191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Classification Metrics (LDA as Classifier) ===\n",
      "Accuracy      : 0.7616\n",
      "Precision     : 0.9249\n",
      "Recall        : 0.6326\n",
      "F1-Score      : 0.7514\n",
      "AUC           : 0.8484\n",
      "FAR (False Alarm Rate): 0.0679\n",
      "\n",
      "=== Confusion Matrix ===\n",
      "TN: 9051, FP: 659, FN: 4714, TP: 8118\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "import numpy as np\n",
    "\n",
    "# === 1. Reproducibility ===\n",
    "np.random.seed(42)\n",
    "\n",
    "# === 2. Load datasets ===\n",
    "train_df = pd.read_csv(\"KDDTrain_cleaned_binary.csv\")\n",
    "test_df = pd.read_csv(\"KDDTest_cleaned_binary.csv\")\n",
    "\n",
    "# === 3. Split features and labels ===\n",
    "X_train = train_df.iloc[:, :-1]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "\n",
    "# === 4. One-hot encode categorical columns ===\n",
    "X_all = pd.concat([X_train, X_test])\n",
    "X_all_encoded = pd.get_dummies(X_all, drop_first=True)\n",
    "\n",
    "X_train_encoded = X_all_encoded.iloc[:len(X_train), :]\n",
    "X_test_encoded = X_all_encoded.iloc[len(X_train):, :]\n",
    "\n",
    "# === 5. Normalize features ===\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "# === 6. Train LDA classifier ===\n",
    "lda_model = LinearDiscriminantAnalysis()\n",
    "lda_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# === 7. Predict and evaluate ===\n",
    "y_pred = lda_model.predict(X_test_scaled)\n",
    "y_proba = lda_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# === 8. Compute metrics ===\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "far = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "# === 9. Print results ===\n",
    "print(\"=== Classification Metrics (LDA as Classifier) ===\")\n",
    "print(f\"Accuracy      : {accuracy:.4f}\")\n",
    "print(f\"Precision     : {precision:.4f}\")\n",
    "print(f\"Recall        : {recall:.4f}\")\n",
    "print(f\"F1-Score      : {f1:.4f}\")\n",
    "print(f\"AUC           : {auc:.4f}\")\n",
    "print(f\"FAR (False Alarm Rate): {far:.4f}\")\n",
    "\n",
    "print(\"\\n=== Confusion Matrix ===\")\n",
    "print(f\"TN: {tn}, FP: {fp}, FN: {fn}, TP: {tp}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd46c784-5c65-41cf-8215-c19b978c8d17",
   "metadata": {},
   "source": [
    "## LDA (with SMOTE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a217b43-e744-4ea7-ae95-81d36a1f6286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3937/3937\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 326us/step\n",
      "\u001b[1m705/705\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 350us/step\n",
      "\n",
      "=== SMOTE + LDA Classifier ===\n",
      "Accuracy      : 0.7632\n",
      "Precision     : 0.9238\n",
      "Recall        : 0.6365\n",
      "F1-Score      : 0.7537\n",
      "AUC           : 0.8525\n",
      "FAR           : 0.0694\n",
      "Confusion Matrix: TN=9036, FP=674, FN=4664, TP=8168\n",
      "\n",
      "=== SMOTE + AE + LDA + LogisticRegression ===\n",
      "Accuracy      : 0.7451\n",
      "Precision     : 0.9117\n",
      "Recall        : 0.6114\n",
      "F1-Score      : 0.7319\n",
      "AUC           : 0.8669\n",
      "FAR           : 0.0783\n",
      "Confusion Matrix: TN=8950, FP=760, FN=4987, TP=7845\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, roc_auc_score, confusion_matrix\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# === 1. Reproducibility ===\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# === 2. Load datasets ===\n",
    "train_df = pd.read_csv(\"KDDTrain_cleaned_binary.csv\")\n",
    "test_df = pd.read_csv(\"KDDTest_cleaned_binary.csv\")\n",
    "X_train = train_df.iloc[:, :-1]\n",
    "y_train = train_df.iloc[:, -1]\n",
    "X_test = test_df.iloc[:, :-1]\n",
    "y_test = test_df.iloc[:, -1]\n",
    "\n",
    "# === 3. One-hot encode categorical features ===\n",
    "X_all = pd.concat([X_train, X_test])\n",
    "X_all_encoded = pd.get_dummies(X_all, drop_first=True)\n",
    "X_train_encoded = X_all_encoded.iloc[:len(X_train), :]\n",
    "X_test_encoded = X_all_encoded.iloc[len(X_train):, :]\n",
    "\n",
    "# === 4. Normalize ===\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
    "X_test_scaled = scaler.transform(X_test_encoded)\n",
    "\n",
    "# === 5. Scenario A: SMOTE + LDA as classifier ===\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "lda_clf = LinearDiscriminantAnalysis()\n",
    "lda_clf.fit(X_train_smote, y_train_smote)\n",
    "y_pred_lda = lda_clf.predict(X_test_scaled)\n",
    "y_proba_lda = lda_clf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# === 6. Scenario B: SMOTE + AE + LDA + LR ===\n",
    "# Train autoencoder\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "encoding_dim = 32\n",
    "\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "decoded = Dense(64, activation='relu')(encoded)\n",
    "output_layer = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs=input_layer, outputs=output_layer)\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "autoencoder.fit(X_train_scaled, X_train_scaled, epochs=20, batch_size=256,\n",
    "                shuffle=True, validation_split=0.1, verbose=0)\n",
    "\n",
    "X_train_embedded = encoder.predict(X_train_scaled)\n",
    "X_test_embedded = encoder.predict(X_test_scaled)\n",
    "\n",
    "# Apply SMOTE to embedding\n",
    "X_embed_smote, y_embed_smote = sm.fit_resample(X_train_embedded, y_train)\n",
    "\n",
    "# Apply LDA on embedding after SMOTE\n",
    "lda_proj = LinearDiscriminantAnalysis(n_components=1)\n",
    "X_embed_lda_train = lda_proj.fit_transform(X_embed_smote, y_embed_smote)\n",
    "X_embed_lda_test = lda_proj.transform(X_test_embedded)\n",
    "\n",
    "# Train classifier (e.g., LR) on reduced embedding\n",
    "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
    "lr.fit(X_embed_lda_train, y_embed_smote)\n",
    "y_pred_proj = lr.predict(X_embed_lda_test)\n",
    "y_proba_proj = lr.predict_proba(X_embed_lda_test)[:, 1]\n",
    "\n",
    "# === 7. Evaluation function ===\n",
    "def evaluate(name, y_true, y_pred, y_proba):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"Precision\": precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y_true, y_pred, zero_division=0),\n",
    "        \"F1-Score\": f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"AUC\": roc_auc_score(y_true, y_proba),\n",
    "        \"FAR\": fp / (fp + tn) if (fp + tn) > 0 else 0,\n",
    "        \"ConfusionMatrix\": (tn, fp, fn, tp)\n",
    "    }\n",
    "\n",
    "results = [\n",
    "    evaluate(\"SMOTE + LDA Classifier\", y_test, y_pred_lda, y_proba_lda),\n",
    "    evaluate(\"SMOTE + AE + LDA + LogisticRegression\", y_test, y_pred_proj, y_proba_proj)\n",
    "]\n",
    "\n",
    "# === 8. Print results ===\n",
    "for res in results:\n",
    "    print(f\"\\n=== {res['Model']} ===\")\n",
    "    print(f\"Accuracy      : {res['Accuracy']:.4f}\")\n",
    "    print(f\"Precision     : {res['Precision']:.4f}\")\n",
    "    print(f\"Recall        : {res['Recall']:.4f}\")\n",
    "    print(f\"F1-Score      : {res['F1-Score']:.4f}\")\n",
    "    print(f\"AUC           : {res['AUC']:.4f}\")\n",
    "    print(f\"FAR           : {res['FAR']:.4f}\")\n",
    "    tn, fp, fn, tp = res[\"ConfusionMatrix\"]\n",
    "    print(f\"Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5f2aeb-37ba-4618-857a-8d52ff7a37d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
